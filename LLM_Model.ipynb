{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNCJmPr1fBvsL4o3CqLKj9c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"socUpiq0iB7w","executionInfo":{"status":"ok","timestamp":1709398990170,"user_tz":-330,"elapsed":22113,"user":{"displayName":"Lokesh Sharma","userId":"03376890974234510214"}},"outputId":"d124d0f1-ffe5-4c0b-9680-70cc37c3e513"},"outputs":[{"output_type":"stream","name":"stdout","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!pip install -q torch datasets\n","!pip install -q accelerate==0.21.0 \\\n","                peft==0.4.0 \\\n","                bitsandbytes==0.40.2 \\\n","                transformers==4.31.0 \\\n","                trl==0.4.7\n","# !pip install transformers optimum accelerate peft trl auto-gptq bitsandbytes datasets==2.17.0\n","import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    pipeline\n",")\n","# from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import time\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Define global variables\n","input_tokens = 128\n","output_tokens = 128\n","total_tokens_per_batch = input_tokens + output_tokens\n","concurrency = 32\n","\n","# Load the model and tokenizer\n","def optimize_model(model_name):\n","    # model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map='auto', torch_dtype=torch.bfloat16)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto', torch_dtype=torch.bfloat16)\n","    return model\n","\n","# Function to run model inference\n","def run_model_inference(model, tokenizer, prompt):\n","    start_time = time.time()\n","    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    generated_ids = model.generate(**model_input, max_new_tokens=128)\n","    end_time = time.time()\n","    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    inference_time = end_time - start_time\n","    print(\"Time Taken ::\",inference_time)\n","    print(\"Response:: \",response)\n","    print(\"\\n\\n\")\n","    # from here we can insert response into Database and return to frontend side\n","\n","\n"],"metadata":{"id":"2lqQK-RVi-Pt","executionInfo":{"status":"ok","timestamp":1709398968088,"user_tz":-330,"elapsed":8107,"user":{"displayName":"Lokesh Sharma","userId":"03376890974234510214"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from concurrent.futures import ThreadPoolExecutor\n","import threading\n","\n","def main():\n","    with ThreadPoolExecutor(max_workers=32) as executor:\n","        while True:\n","          try:\n","            model_path = input(\"Enter Hugging Face model path: \")\n","            model = optimize_model(model_path)\n","            tokenizer = AutoTokenizer.from_pretrained(model_path)\n","            question = input(\"Enter prompt: \")\n","            if question.lower().strip() == 'exit' or question.lower().strip() == 'quit':\n","                break\n","\n","            print(\"Currently processing question:\", question)\n","            print(\"Active thread count:\", threading.active_count())\n","\n","            if threading.active_count() < 32:\n","                print(\"While active thread:\", threading.active_count())\n","                executor.submit(run_model_inference, model, tokenizer, question)\n","          except :\n","            print(\"Model not loaded\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Binifc2anYzt","outputId":"19288632-ab52-4bdc-c39b-5f9eb1062230"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model not loaded\n","Model not loaded\n","Model not loaded\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1L9z7_cvqAO9"},"execution_count":null,"outputs":[]}]}